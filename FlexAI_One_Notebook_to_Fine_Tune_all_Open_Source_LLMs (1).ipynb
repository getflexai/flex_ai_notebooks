{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uEmjXNVWTCQv"
      },
      "source": [
        "# Welcome to FlexAI!\n",
        "\n",
        "\n",
        "We found the process of fine-tuning and inference really complicated, from initializing a GPU server to setting the app's env installions, OOMs, prompt templates, multi-GPU, and more. We decided to build a platform that helps you do fine-tuning and inference for over 60+ open-source LLMs, in a single API, so your teams can save up to 70% of the time on non-important stuff, all serverless (like Unsloth/Axololt but as a SaaS).\n",
        "\n",
        "Some of the features in FlexAI platform:\n",
        "\n",
        "- Serverless fine-tuning and inference\n",
        "- Live time and cost estimations\n",
        "- Checkpoints management\n",
        "- Lora and multi-Lora\n",
        "- Target inference validations\n",
        "- OpenAI compatible Endpoints API in a single click\n",
        "- Playground\n",
        "\n",
        "Check us out - https://getflex.ai/\n",
        "\n",
        "# Setup\n",
        "(We suggest using CPU for this colab, since the compute is remote)\n",
        "\n",
        "To install FlexAI, run the following command:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qjodw2gJ_7Qa"
      },
      "outputs": [],
      "source": [
        "!pip install flex_ai openai"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8soKdod3EFcI"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0z1BVvAAUIE6"
      },
      "source": [
        "## Create a FlexAI client and choose your model\n",
        "You need to sign up first at https://app.getflex.ai/sign-in to get an API key (You have 10$ for training and inference for free)\n",
        "\n",
        "To get the API key go to \"Settings\" -> \"API Keys\" -> Paste your API key to the form.\n",
        "\n",
        "Then, choose the model that you want to fine-tune from the dropdown. If you need any other HuggingFace model, please contact-us.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2YdAyb98Af98"
      },
      "outputs": [],
      "source": [
        "from flex_ai import FlexAI\n",
        "\n",
        "# you can get your api key from https://app.getflex.ai/settings/api-keys\n",
        "\n",
        "# @title\n",
        "API_KEY = \"\" # @param {\"type\":\"string\"}\n",
        "MODEL = \"meta-llama/Llama-3.2-3B-Instruct\" # @param ['TinyLlama/TinyLlama_v1.1', 'meta-llama/Meta-Llama-3-70B-Instruct', 'microsoft/Phi-3-medium-128k-instruct', 'Qwen/Qwen2-1.5B', 'mistralai/Mixtral-8x7B-Instruct-v0.1', 'Qwen/Qwen2-57B-A14B', 'microsoft/Phi-3-small-8k-instruct', 'meta-llama/Meta-Llama-3-8B-Instruct', 'google/gemma-2-27b-it', 'Qwen/Qwen2-1.5B-Instruct', '01-ai/Yi-1.5-9B', '01-ai/Yi-1.5-9B-Chat', 'google/gemma-2-2b-it', 'google/gemma-2-9b-it', 'ai21labs/Jamba-v0.1', '01-ai/Yi-1.5-6B', '01-ai/Yi-1.5-34B-Chat', 'Qwen/Qwen2-72B-Instruct', 'Qwen/Qwen2-72B', 'google/gemma-2-27b', 'Qwen/Qwen2-7B', 'Qwen/Qwen2-7B-Instruct', 'microsoft/Phi-3-mini-128k-instruct', 'microsoft/Phi-3-medium-4k-instruct', 'microsoft/Phi-3-mini-4k-instruct', 'Qwen/Qwen2-0.5B-Instruct', 'microsoft/Phi-3-small-128k-instruct', 'meta-llama/Meta-Llama-3-8B', 'meta-llama/Meta-Llama-3-70B', 'mistralai/Mistral-7B-Instruct-v0.3', 'deepseek-ai/DeepSeek-Coder-V2-Lite-Base', 'CohereForAI/c4ai-command-r-v01', 'Qwen/Qwen2-57B-A14B-Instruct', 'mistralai/Mistral-Nemo-Instruct-2407', 'google/gemma-2-9b', '01-ai/Yi-1.5-34B-Chat-16K', '01-ai/Yi-1.5-34B-32K', 'mistralai/Mistral-Nemo-Base-2407', 'Qwen/Qwen2-0.5B', '01-ai/Yi-1.5-9B-Chat-16K', '01-ai/Yi-1.5-34B', 'internlm/internlm2_5-7b', 'Qwen/Qwen2.5-Coder-7B-Instruct', '01-ai/Yi-1.5-9B-32K', '01-ai/Yi-1.5-6B-Chat', 'internlm/internlm2_5-7b-chat', 'meta-llama/Meta-Llama-3.1-8B', 'meta-llama/Meta-Llama-3.1-8B-Instruct', 'meta-llama/Meta-Llama-3.1-70B', 'meta-llama/Meta-Llama-3.1-70B-Instruct', 'meta-llama/Llama-3.2-3B-Instruct', 'meta-llama/Llama-3.2-1B-Instruct']\n",
        "\n",
        "client=FlexAI(api_key=API_KEY)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Hl9K22DU0Iz"
      },
      "source": [
        "## Create your first dataset\n",
        "If you dont have any data, you can unmark this code and you will download a sample of train and eval datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-q9nB7cyvuh_"
      },
      "outputs": [],
      "source": [
        "# If you dont have any files to upload , you can this data example\n",
        "# Download and Create instruction dataset\n",
        "# train_url = \"https://secure.getflex.ai/storage/v1/object/public/files/sample_data/train.jsonl\"\n",
        "# eval_url = \"https://secure.getflex.ai/storage/v1/object/public/files/sample_data/eval.jsonl\"\n",
        "\n",
        "# !wget {train_url} -O train.jsonl\n",
        "# !wget {eval_url} -O eval.jsonl"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Upload your own Dataset\n",
        "\n",
        "You can check the supported datasets formats here: https://docs.getflex.ai/quickstart#upload-your-first-dataset"
      ],
      "metadata": {
        "id": "KNVAiWciZ7o6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, HTML\n",
        "import json\n",
        "import time\n",
        "\n",
        "def upload_and_process_file(file_type):\n",
        "    progress = widgets.FloatProgress(value=0, min=0, max=100, description=f'Uploading {file_type}:')\n",
        "    display(progress)\n",
        "\n",
        "    uploaded = files.upload()\n",
        "\n",
        "    for filename, content in uploaded.items():\n",
        "        # Simulate processing\n",
        "        for i in range(100):\n",
        "            time.sleep(0.05)  # Simulate some work\n",
        "            progress.value = i + 1\n",
        "\n",
        "        # Process the file (in this case, just count the lines)\n",
        "        lines = content.decode('utf-8').split('\\n')\n",
        "        num_lines = len(lines)\n",
        "\n",
        "        print(f\"{file_type.capitalize()} file '{filename}' uploaded and processed. It contains {num_lines} lines.\")\n",
        "\n",
        "        # You can add more processing here as needed\n",
        "\n",
        "    return uploaded\n",
        "\n",
        "# Create buttons\n",
        "train_button = widgets.Button(description=\"Upload Train File\")\n",
        "eval_button = widgets.Button(description=\"Upload Eval File\")\n",
        "\n",
        "# Define button click handlers\n",
        "def on_train_button_clicked(b):\n",
        "    train_data = upload_and_process_file('train')\n",
        "\n",
        "def on_eval_button_clicked(b):\n",
        "    eval_data = upload_and_process_file('eval')\n",
        "\n",
        "# Attach click handlers to buttons\n",
        "train_button.on_click(on_train_button_clicked)\n",
        "eval_button.on_click(on_eval_button_clicked)\n",
        "\n",
        "# Display the interface\n",
        "display(HTML(\"<h2>Upload Your Custom .jsonl Files</h2>\"))\n",
        "display(HTML(\"<p>Click the buttons below to upload your train and eval files.</p>\"))\n",
        "display(train_button, eval_button)"
      ],
      "metadata": {
        "id": "ibqvMt7qY-Y5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RiXPwLmHBOCV"
      },
      "outputs": [],
      "source": [
        "# Eval is optional, but recommended\n",
        "dataset = client.create_dataset(\"My First Flex Dataset\",\"train.jsonl\",\"eval.jsonl\")\n",
        "print(\"Check the Datasets: https://app.getflex.ai/datasets\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T4dAtsk0WBRj"
      },
      "source": [
        "## Create a Fine Tune Task"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Nin_jWEBjfY"
      },
      "outputs": [],
      "source": [
        "# Start a fine-tuning task\n",
        "task = client.create_finetune(\n",
        "    name=\"My Task New\",\n",
        "    dataset_id=dataset[\"id\"],\n",
        "    model=MODEL,\n",
        "    n_epochs=10,\n",
        "    train_with_lora=True,\n",
        "    lora_config={\n",
        "        \"lora_r\": 64,\n",
        "        \"lora_alpha\": 8,\n",
        "        \"lora_dropout\": 0.1\n",
        "    },\n",
        "    n_checkpoints_and_evaluations_per_epoch=1,\n",
        "    batch_size=4,\n",
        "    learning_rate=0.0001,\n",
        "    save_only_best_checkpoint=False\n",
        ")\n",
        "\n",
        "# You can now monitor the task status and results through the Flex AI dashboard\n",
        "print(f\"Fine-tuning task created with ID: {task['id']}\")\n",
        "print(\"Check the Tasks Page for status updates: https://app.getflex.ai/tasks\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hSQXV1cIWKrq"
      },
      "source": [
        "## Show Live Logs and wait for the task to be completed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fr-mPTejXwW6"
      },
      "outputs": [],
      "source": [
        "client.wait_for_task_completion(task_id=task[\"id\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TYG-2TvJWQQc"
      },
      "source": [
        "## Wait for last checkpoint to be uploaded"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "To3j9hzEX0gF"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "while True:\n",
        "    checkpoints = client.get_task_checkpoints(task_id=task[\"id\"])\n",
        "    if checkpoints and checkpoints[-1][\"stage\"] == \"FINISHED\":\n",
        "        last_checkpoint = checkpoints[-1]\n",
        "        checkpoint_list = [{\n",
        "            \"id\": last_checkpoint[\"id\"],\n",
        "            \"name\": \"step_\" + str(last_checkpoint[\"step\"])\n",
        "        }]\n",
        "        break\n",
        "    time.sleep(10)  # Wait 10 seconds before checking again"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "stjHtU_KWS4t"
      },
      "source": [
        "##Create an OpenAI-compatible endpoint for your Fine Tuned Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UzrBeHzxX5PY"
      },
      "outputs": [],
      "source": [
        "endpoint_id = client.create_multi_lora_endpoint(name=\"My Endpoint New\", lora_checkpoints=checkpoints_list, compute=\"A100-40GB\")\n",
        "endpoint = client.wait_for_endpoint_ready(endpoint_id=endpoint_id)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0RQc6Ww-WZSQ"
      },
      "source": [
        "##Use your fine tuned Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ApAEy_j2YCDN"
      },
      "outputs": [],
      "source": [
        "from openai import OpenAI\n",
        "\n",
        "openai_endpoint_url = endpoint[\"url\"]\n",
        "client = OpenAI(\n",
        " api_key=API_KEY,\n",
        " base_url=f\"{openai_endpoint_url}/v1\"\n",
        ")\n",
        "completion = client.completions.create(\n",
        " model=MODEL,\n",
        " prompt=\"Translate the following English text to French\",\n",
        " max_tokens=60\n",
        ")\n",
        "print(completion.choices[0].text)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}